\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[russian]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[a4rpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath, amssymb, amsthm}
\usepackage{indentfirst}
\usepackage{graphicx, float}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\setlength{\parskip}{5pt}
\setlength{\parindent}{20}
\title{Байесовы нейронные сети}
\author{Басов Дмитрий Константинович}
\date{}
\begin{document}
\maketitle

\section{Аннотация}
$N(\mu, \sigma^2)$ — нормальное распределение

$\mathcal{L}$ — Evidence Lower Bound (ELBO)

$KL(q || p) = \int_{}{} q(\bold{Z}) \cdot \ln{\dfrac{q(\bold{Z})}{p(\bold{Z})}} d\bold{Z}$ — расстояние Кульбака — Лейблера

$\bold{x}$ — вектор признаков

$\bold{y}$ — таргет

$D$ — датасет — пары значений \{$\bold{x_i}$, $\bold{y_i}$\}, где $i = 1, \dots, L$

$\bold{W}$ — параметры модели — случайная величина размерности M

$p(D | \bold{W}) = \prod_{i=1}^{L} p(\bold{y_i} | \bold{x_i}, \bold{W})$ — правдоподобие (likelihood)

$p(\bold{W})$ — априорное распределение параметров модели (prior)

$p(\bold{W}| D)$ — апостериорное распределение параметров модели (posterior)

$p(D)$ — маргинальная вероятность датасета (evidence)

$q(\bold{W})$ — аппроксимация апостериорного распределения параметров модели

$p(\bold{W}, D) =
p(D | \bold{W}) \cdot p(\bold{W}) =
p(\bold{W}| D)\cdot p(D)$
— совместная вероятность параметров и данных

\section{Постановка задачи}
Постановка задачи следующая — у нас есть датасет $D$ и наша цель — смоделировать распределение $p(\bold{y} | \bold{x}, D)$. То есть мы хотим получить распределение вероятностей таргета $\bold{y}$ для неразмеченных $\bold{x}$ используя датасет D. Сделаем следующие преобразования:

$p(\bold{y} | \bold{x}, D) =
\int_{}{} p(\bold{y}, \bold{W} | \bold{x}, D) d\bold{W} =
\int_{}{} p(\bold{y} | \bold{W}, \bold{x}, D) \cdot p(\bold{W} | \bold{x}, D) d\bold{W} = 
\int_{}{} p(\bold{y} | \bold{W}, \bold{x}) \cdot p(\bold{W} | D) d\bold{W}$

Получим выражение для $p(\bold{W}| D)$, используя формулу Байеса:

$p(\bold{W}| D) =
\dfrac{p(\bold{W}, D)}{p(D)} =
\dfrac{p(\bold{W}, D)}{\int_{}{} p(\bold{W}, D) d\bold{W}} =
\dfrac{p(D | \bold{W}) \cdot p(\bold{W})}{\int_{}{} p(D | \bold{W}) \cdot p(\bold{W}) d\bold{W}} $

Для аппроксимации распределения ответов модели можно воспользоваться методом Монте — Карло — взять сэмпл весов $\hat{\bold{W}}$ из $p(\bold{W}| D)$, прогнать их через модель и получить $\hat{\bold{y}}$. Однако для этого необходимо уметь сэмплировать из распределения $p(\bold{W}| D)$.

Получить аналитическое решение можно только в очень ограниченном числе случаев. Существует возможность семплировать из $p(\bold{W}| D)$, используя методы Монте — Карло для марковских цепей (MCMC). Однако для больших датасетов и большого числа параметров это становится технически сложно. Альтернативный подход к решению таких задач — аппроксимация распределения $p(\bold{W}| D)$ распределением $q(\bold{W})$, из которого сэмплировать намного проще.


\section{Вариационный вывод для нейронной сети}
Запишем выражение ELBO для распределения $q(\bold{W})$ и преобразуем его используя тождество $p(\bold{W}, D) = p(\bold{W}| D)\cdot p(D)$:

$
\mathcal{L}(q(\bold{W})) =
\int_{}{} q(\bold{W}) \cdot \ln{\dfrac{p(\bold{D}, \bold{W})}{q(\bold{W})}} d\bold{W} =
\int_{}{} q(\bold{W}) \cdot \ln{\dfrac{p(\bold{W}| D) \cdot p(D)}{q(\bold{W})}} d\bold{W} = 
\ln{p(D)} \cdot \int_{}{} q(\bold{W}) d\bold{W} - \int_{}{} q(\bold{W})\cdot \ln{\dfrac{q(\bold{W})}{p(\bold{W}| D)}} d\bold{W} =
\ln{p(D)} - KL(q(\bold{W}) || p(\bold{W}| D))
$

Из равенства $\mathcal{L}(q(\bold{W})) = ln(p(D)) - KL(q(\bold{W}) || p(\bold{W}| D))$ видно, что максимизируя $\mathcal{L}(q(\bold{W}))$, мы не только максимизируем $\ln {p(D)}$, но и минимизируем $KL(q(\bold{W}) || p(\bold{W}| D))$. То есть распределение $q(\bold{W})$ будет приближаться к распределению $p(\bold{W}| D)$.

Будем максимизировать $\mathcal{L}(q(\bold{W}))$. Преобразуем выражение для $\mathcal{L}(q(\bold{W}))$, используя тождество
$p(\bold{W}, D) = p(D | \bold{W}) \cdot p(\bold{W})$:

$
\mathcal{L}(q(\bold{W})) =
\int_{}{} q(\bold{W}) \cdot \ln{\dfrac{p(\bold{D}, \bold{W})}{q(\bold{W})}} d\bold{W} =
\int_{}{} q(\bold{W}) \cdot \ln{\dfrac{p(D | \bold{W}) \cdot p(\bold{W})}{q(\bold{W})}} d\bold{W} = 
\int_{}{} q(\bold{W}) \cdot \ln{p(D | \bold{W})} d\bold{W} - \int_{}{} q(\bold{W}) \cdot \ln{\dfrac{q(\bold{W})}{p(\bold{W})}} = 
\int_{}{} q(\bold{W}) \cdot \ln{p(D | \bold{W})} d\bold{W} - KL(q(\bold{W}) || p(\bold{W}))
$

Для дальнейшнего вывода положим, что распределения $p(\bold{W})$ и $q(\bold{W})$ являются нормальными с диагональными матрицами ковариации:

$p(\bold{W}) = N(\bold{W} | \bold{0}, \pmb{\sigma_{p(\bold{W})}}^{2} \cdot \bold{I})$, где $\pmb{\sigma_{p(\bold{W})}}$ — вектор длины M

$q(\bold{W}) = N(\bold{W} | \pmb{\theta}, \pmb{\sigma_{q(\bold{W})}}^{2} \cdot \bold{I})$, где $\pmb{\theta}$ и $\pmb{\sigma_{q(\bold{W})}}$ — вектора длины M

Так как распределения $p(\bold{W})$ и $q(\bold{W})$ являются нормальными, то $KL(q(\bold{W}) || p(\bold{W}))$ можно посчитать аналитически:

$
KL(q(\bold{W}) || p(\bold{W})) = 
\dfrac{1}{2}\sum_{k=1}^{M}(\dfrac{\sigma_{{q(W)_{k}}}^2}{\sigma_{{p(W)_{k}}}^2} + \dfrac{\theta_{k}^2}{\sigma_{{p(W)_{k}}}^2} - \ln{\dfrac{\sigma_{{q(W)_{k}}}^2}{\sigma_{{p(W)_{k}}}^2}} - 1)
$

Априорное распределение параметров модели определяется параметром $\pmb{\sigma_{p(\bold{W})}}$. Воспользуемся техникой эмперического Байеса — нахождения параметров априорного распределения из данных. Посчитаем
$\dfrac{d\mathcal{L}(q(\bold{W}))}{d ({\pmb{\sigma_{p(\bold{W})}^{-2}}})}$:

$
\dfrac{d\mathcal{L}(q(\bold{W}))}{d ({\pmb{\sigma_{p(\bold{W})}^{-2}}})} =
\dfrac{d (\int_{}{} q(\bold{W}) \cdot \ln{p(D | \bold{W})} d\bold{W} - KL(q(\bold{W}) || p(\bold{W})))}{d ({\pmb{\sigma_{p(\bold{W})}^{-2}}})} =
- \dfrac{d (KL(q(\bold{W}) || p(\bold{W})))}{d ({\pmb{\sigma_{p(\bold{W})}^{-2}}})}$

$
\dfrac{d\mathcal{L}(q(\bold{W}))}{d ({\pmb{\sigma_{p(\bold{W})}^{-2}}})} =
-\dfrac{1}{2}\sum_{k=1}^{M}(\sigma_{{q(W)_{k}}}^2 + \theta_{k}^2 - \sigma_{{p(W)_{k}}}^2)
$

Приравняв производную к нулю, получим:

$\pmb{\sigma_{p(\bold{W})}^{2}} = \pmb{\theta^{2}} + \pmb{\sigma_{q(\bold{W})}^{2}}$

Подставив полученное выражение в $KL(q(\bold{W}) || p(\bold{W}))$, получим:

$
KL(q(\bold{W}) || p(\bold{W})) = 
\dfrac{1}{2}\sum_{k=1}^{M}\ln({1 + \dfrac{\theta_{k}^2}{\sigma_{{q(W)_{k}}}^2}})
$

Чтобы избежать неопределенности $\dfrac{0}{0}$ и переписать выражение в векторном виде, сделаем следующую замену переменных:

$\pmb{\theta} = \pmb{\gamma} \cdot \pmb{\sigma_{q(\bold{W})}}$

$\pmb{\nu} = \ln{(1 + \pmb{\gamma}^2)}$

Так как обучение модели будет производится с помощью градиентных методов, сделаем следующую замену переменных, чтобы
$\pmb{\sigma_{q(\bold{W})}}$ была всегда положительна:

$\pmb{\sigma_{q(\bold{W})}} = \ln{(1+\exp(\pmb{\rho}))} = Softplus(\pmb{\rho})$

Таким образом, функция потерь будет иметь следующий вид:

$
Loss(\pmb{\rho}, \pmb{\gamma}) =
- \dfrac{\mathcal{L}(q(\bold{W}))}{L} =
\int_{}{} N(\bold{W} | \pmb{\theta}, \pmb{\sigma_{q(\bold{W})}}^{2} \cdot \bold{I}) \sum_{i=1}^{L}{\dfrac{-\ln{p( \bold{y_{i}} | \bold{x_{i}}, \bold{W})}}{L}} d\bold{W} + \dfrac{\pmb{\nu}^{T} \pmb{\nu}}{2L}
$, где:

$\pmb{\nu} = \ln{(1 + \pmb{\gamma}^2)}$

$\pmb{\theta} = \pmb{\gamma} \cdot \pmb{\sigma_{q(\bold{W})}}$

$\pmb{\sigma_{q(\bold{W})}} = Softplus(\pmb{\rho})$

\section{Алгоритм обучения}
Задаем шаг градиентного спуска $\alpha$ и инициализируем параметры распределения $q(\bold{W})$ — $\pmb{\rho} \leftarrow \bold{1}$ и $\pmb{\gamma} \leftarrow \bold{0}$. Затем повторяем, пока не достигнем критерия остановки:
\begin{enumerate}
    \item $\pmb{\sigma} \leftarrow Softplus(\pmb{\rho})$
    \item $\pmb{\theta} \leftarrow \pmb{\gamma} \cdot \pmb{\sigma}$
    \item $\pmb{\nu} \leftarrow \ln{(1 + \pmb{\gamma}^2)}$
    \item $\hat{\bold{W}} \leftarrow N(0, 1)$ — сэмплируем случайные веса
    \item $\hat{\bold{W}} \leftarrow \hat{\bold{W}} \cdot \pmb{\sigma} + \pmb{\theta}$ — репараметризация
    \item $l \leftarrow \dfrac{\pmb{\nu}^{T} \pmb{\nu}}{2L} -\sum_{i=1}^{L}{\dfrac{\ln{p( \bold{y_{i}} | \bold{x_{i}}, \bold{\hat{W}})}}{L}}$ — считаем функцию потерь
    \item $\pmb{\rho} \leftarrow \pmb{\rho} - \alpha \dfrac{d l}{d \pmb{\rho}}$
    \item $\pmb{\gamma} \leftarrow \pmb{\gamma} - \alpha \dfrac{d l}{d \pmb{\gamma}}$
\end{enumerate}

Если моя задумка верна, то лишние веса модели должны выпилиться, то есть соответсвующие им $\theta$ и $\sigma$ должны занулиться.

\end{document}